# HW06 – Report

> Файл: `homeworks/HW06/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.
---
## 1. Dataset

- Какой датасет выбран: `S06-hw-dataset-04.csv`
- Размер: (25000, 62)
- Целевая переменная: `target` (два класса: 0, 1; доли - 0: 0.9508, 1: 0.0492)
- Признаки: только числовые (float64: 60, int64: 2)
---
## 2. Protocol

- Разбиение: train/test = 75% / 25%, `random_state = 42`
- Для сохранения долей классов использовалась стратификация (`stratify = y`)
- Подбор гиперпараметров:
  - выполнялся **только на train-выборке**
  - использовалась стратифицированная k-fold CV (`k = 5`)
- Test-выборка использовалась **один раз** для финальной оценки моделей

Метрики качества:
- **accuracy** — базовая метрика
- **F1-score** — учитывает дисбаланс классов
- **ROC-AUC** — основная метрика выбора модели, так как отражает качество ранжирования
  и устойчива к дисбалансу классов
---
## 3. Models

В работе были реализованы и сравнены следующие модели:

- **DummyClassifier** (`most_frequent`) - наивный baseline
- **LogisticRegression** - использовалась через `Pipeline(StandardScaler → LogisticRegression)`
- **DecisionTreeClassifier** - контроль сложности осуществлялся с помощью `max_depth` и `min_samples_leaf`
- **RandomForestClassifier** - подбирались параметры: `n_estimators`, `max_depth`, `min_samples_leaf`, `max_features`
- **HistGradientBoostingClassifier** - подбирались параметры: `learning_rate`, `max_depth`, `max_iter`, `min_samples_leaf`

Для всех моделей с подбором гиперпараметров использовался `GridSearchCV`
с кросс-валидацией на train-выборке.
---
## 4. Results

### Финальные метрики на test-выборке

| Модель                  | Accuracy | F1-score | ROC-AUC |
|-------------------------|----------|----------|---------|
| DummyClassifier         | 0.9509   | 0.0000   | 0.5000  |
| LogisticRegression      | 0.9627   | 0.4131   | 0.8397  |
| DecisionTreeClassifier  | 0.9685   | 0.5887   | 0.8280  |
| RandomForestClassifier  | 0.9730   | 0.6219   | 0.8964  |
| HistGradientBoosting    | 0.9792   | 0.7368   | 0.9024  |

Финальные метрики для всех моделей также сохранены в файле `artifacts/metrics_test.json`.

### Выбор лучшей модели

В качестве основного критерия использовалась метрика **ROC-AUC**, поскольку задача
характеризуется выраженным дисбалансом классов, а accuracy в таких условиях может
давать вводящую в заблуждение оценку качества.

Лучшей моделью стала **HistGradientBoostingClassifier**, показавшая наивысшее значение
ROC-AUC (≈ 0.90), а также максимальный F1-score (≈ 0.74). Это указывает на её способность
эффективно ранжировать объекты и корректно выявлять объекты меньшинства.

Ансамблевые методы в целом существенно превосходят baseline-модели, а boosting-модель
демонстрирует наилучший баланс между качеством и устойчивостью на дисбалансных данных.
---

## 5. Analysis

### Устойчивость

Для оценки устойчивости моделей были выполнены несколько прогонов (не менее 5)
с различными значениями `random_state` для ансамблевых моделей. Результаты показали,
что порядок моделей по качеству сохраняется: boosting- и forest-модели стабильно
превосходят одиночное дерево решений и baseline-модели.

Разброс значений ROC-AUC для ансамблевых моделей оказался небольшим, что указывает
на их устойчивость к случайности разбиения данных. В то же время одиночное дерево
решений демонстрирует более заметную вариативность, что характерно для моделей
с высокой чувствительностью к обучающей выборке.

---

### Ошибки

Confusion matrix для лучшей модели (HistGradientBoostingClassifier) показывает
существенное снижение количества ошибок типа false negative по сравнению с
baseline-моделями, что является критически важным свойством для fraud-like задач.

При этом наблюдается умеренное количество false positive, что представляет собой
ожидаемый компромисс между полнотой и точностью при работе с дисбалансными данными.
С точки зрения прикладной задачи такой баланс является предпочтительным, поскольку
пропуск объектов меньшинства более критичен, чем ложные срабатывания.

---

### Интерпретация

Для лучшей модели была рассчитана permutation importance (top-15 признаков).
Анализ показал, что наибольший вклад в качество модели вносят признаки f54 и f25,
значительно превосходящие остальные по величине importance. Эти признаки оказывают
ключевое влияние на итоговые предсказания модели.

Следующая группа признаков (f47, f58, f33, f38) также демонстрирует заметный вклад,
однако их влияние менее выражено. Остальные признаки из топ-15 обладают умеренным
вкладом и, вероятно, участвуют в формировании более тонких нелинейных взаимодействий.

Полученная структура важности соответствует ожиданиям для синтетического
fraud-like датасета, где информация о целевом классе концентрируется в ограниченном
числе информативных признаков, а большая часть признаков вносит вспомогательный
или шумовой вклад. Это объясняет высокую эффективность boosting-модели.


## 6. Conclusion

- Одиночные деревья решений чувствительны к переобучению и нестабильны на сложных и шумных данных.
- Ансамблевые методы существенно повышают качество за счёт агрегации нескольких моделей и снижения дисперсии.
- Boosting-модели особенно эффективны на табличных и дисбалансных данных, так как последовательно исправляют ошибки.
- Accuracy не является надёжной метрикой при дисбалансе классов и может вводить в заблуждение.
- Метрики ROC-AUC и F1-score дают более честную и информативную оценку качества моделей.
- Корректный ML-протокол (подбор гиперпараметров только на train и однократная оценка на test) критически важен для достоверного сравнения моделей.

